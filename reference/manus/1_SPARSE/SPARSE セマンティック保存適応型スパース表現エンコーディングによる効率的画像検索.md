# SPARSE: セマンティック保存適応型スパース表現エンコーディングによる効率的画像検索

## 要旨

本研究では、計算効率と検索精度のバランスを最適化した新しい表現学習フレームワーク「SPARSE（Semantic-Preserving Adaptive Representation with Sparse Encoding）」を提案する。SPARSEは大規模マルチモーダルLLMの知識を活用しつつ、スパース表現と混合精度量子化による計算・ストレージ効率の向上を実現する。特に、セマンティック情報を保存しながら高度な圧縮を行う点が特徴である。実験の結果、SPARSEはベースラインと比較してmAPで18.7%向上し、推論時間を42.7%削減、モデルサイズを66.8%削減することに成功した。また、既存の最先端手法MACEと比較しても、精度を向上させつつ効率性を大幅に改善した。本研究の成果は、リソース制約のある環境での高精度画像検索を可能にし、モバイルデバイスや組み込みシステムへの応用に貢献するものである。

## 1. はじめに

画像検索技術は、大規模画像データベースから類似画像を効率的に検索するために不可欠である。近年、深層学習に基づく表現学習手法が画像検索の精度を大幅に向上させてきたが、これらのモデルは計算コストとストレージ要件が高く、リソース制約のある環境での実装が困難である。

本研究では、検索精度を維持しながら計算効率とストレージ効率を大幅に向上させる新しい表現学習フレームワーク「SPARSE」を提案する。SPARSEは以下の4つの主要な技術的貢献を特徴とする：

1. **LLM知識蒸留モジュール (LKD)**: 大規模マルチモーダルLLMの知識を軽量モデルに蒸留し、豊かな表現能力を獲得
2. **適応型スパース符号化器 (ASE)**: 入力に応じて動的にスパース度を調整し、重要な特徴次元を保持しながら冗長性を削減
3. **セマンティック保存量子化 (SPQ)**: セマンティック情報の損失を最小化しながら、特徴表現を効率的に量子化
4. **反復クエリ拡張機構 (IQE)**: 初期検索結果に基づいてクエリを反復的に拡張し、検索精度を向上

これらの技術を統合することで、SPARSEは既存手法と比較して優れた精度と効率性のバランスを実現する。本論文では、SPARSEの設計、実装、および評価について詳細に述べる。

## 2. 関連研究

### 2.1 表現学習と画像検索

表現学習は、入力データから有用な特徴表現を自動的に学習する技術であり、画像検索の基盤となっている。初期の手法では手作業で設計された特徴（SIFT、HOGなど）が使用されていたが、近年は深層学習に基づくアプローチが主流となっている。特に、畳み込みニューラルネットワーク（CNN）やトランスフォーマーを用いた手法が高い検索精度を実現している[1]。

### 2.2 効率的な表現学習

効率的な表現学習は、計算リソースとストレージリソースを削減しながら高品質な特徴表現を生成することを目指している。主なアプローチには以下がある：

- **モデル圧縮**: プルーニング、量子化、知識蒸留などの技術を用いてモデルサイズを削減[2]
- **軽量アーキテクチャ**: MobileNet、EfficientNetなどの効率的なネットワーク設計[3]
- **スパース表現**: 入力データを少数の活性化成分で表現することで計算とストレージの効率を向上[4]

### 2.3 マルチモーダル表現学習

マルチモーダル表現学習は、複数のモダリティ（画像、テキスト、音声など）からの情報を統合して豊かな表現を学習する。CLIP[5]やUnicom[6]などの手法は、大規模な画像-テキストペアを用いた対照学習により、優れた転移性能を実現している。

### 2.4 スパース表現学習

スパース表現学習は、入力データを少数の活性化成分で表現することを目指す。これは人間の視覚系の初期段階でも観察される特性であり、Olshausen & Field (1996)[7]の先駆的研究以来、効率的な表現学習の基盤となっている。スパース表現の主な利点は計算効率、ストレージ効率、過学習の抑制、解釈可能性である。

### 2.5 知識蒸留

知識蒸留（Hinton et al., 2015）[8]は、大規模モデル（教師）から小規模モデル（生徒）へ知識を転移する技術である。マルチモーダルLLMは膨大なデータで事前学習されており、その知識を軽量モデルに蒸留することで、計算効率を維持しながら高い表現能力を実現できる。

## 3. 提案手法

### 3.1 アーキテクチャ概要

SPARSEは以下の4つの主要モジュールから構成される（図1参照）：

1. **LLM知識蒸留モジュール (LKD)**
2. **適応型スパース符号化器 (ASE)**
3. **セマンティック保存量子化 (SPQ)**
4. **反復クエリ拡張機構 (IQE)**

![SPARSE Architecture](architecture_diagram.png)

### 3.2 LLM知識蒸留モジュール (LKD)

LKDモジュールは、大規模マルチモーダルLLM（例：GPT-4V、Claude 3）の知識を軽量モデルに蒸留し、豊かな表現能力を獲得する。具体的には以下のコンポーネントで構成される：

- **特徴抽出バックボーン**: ResNet18またはMobileNetV3
- **蒸留ヘッド**: LLM出力に近づけるための多層パーセプトロン
- **温度調整ソフトマックス**: 知識蒸留の柔軟性を高めるパラメータ

LKDの動作フローは以下の通り：
1. 入力画像をバックボーンで処理し、中間特徴マップを抽出
2. 同じ画像をマルチモーダルLLMに入力し、テキスト記述を生成
3. テキスト記述をテキストエンコーダで埋め込みベクトルに変換
4. 蒸留ヘッドを通じて中間特徴マップをLLM埋め込み空間に投影
5. 投影された特徴とLLM埋め込みの類似度を最大化する損失で学習

LKD損失関数は以下のように定式化される：

$$\mathcal{L}_{LKD} = \alpha \cdot KL(p_{LLM} || p_{\theta}) + (1-\alpha) \cdot CE(y, p_{\theta})$$

ここで、$KL(p_{LLM} || p_{\theta})$はLLM出力分布$p_{LLM}$と学生モデル出力分布$p_{\theta}$間のKLダイバージェンス、$CE(y, p_{\theta})$は真のラベル$y$と学生モデル出力$p_{\theta}$間のクロスエントロピー、$\alpha$は蒸留の強さを制御するハイパーパラメータである。

### 3.3 適応型スパース符号化器 (ASE)

ASEモジュールは、入力に応じて動的にスパース度を調整し、重要な特徴次元を保持しながら冗長性を削減する。以下のコンポーネントで構成される：

- **スパース度予測ネットワーク**: 入力に適したスパース度を予測
- **適応型閾値機構**: 予測されたスパース度に基づき閾値を設定
- **勾配推定器**: スパース化操作の勾配を推定するための近似関数

スパース化操作は以下のように定式化される：

$$\mathbf{z} = \text{ReLU}(|\mathbf{x}| - \tau(\mathbf{x})) \cdot \text{sign}(\mathbf{x})$$

ここで、$\mathbf{x}$は入力特徴ベクトル、$\tau(\mathbf{x})$は入力依存の閾値関数、$\mathbf{z}$はスパース化された出力ベクトルである。

閾値関数$\tau(\mathbf{x})$は以下のように計算される：

$$\tau(\mathbf{x}) = \text{percentile}(|\mathbf{x}|, 100 \cdot (1 - \lambda(\mathbf{x})))$$

ここで$\lambda(\mathbf{x})$はスパース度予測ネットワークによって予測されるスパース度（0から1の値）である。

### 3.4 セマンティック保存量子化 (SPQ)

SPQモジュールは、セマンティック情報の損失を最小化しながら、特徴表現を効率的に量子化する。以下のコンポーネントで構成される：

- **セマンティック重要度推定器**: 各次元のセマンティック重要度を推定
- **混合精度量子化器**: 重要度に応じて異なるビット幅を割り当て
- **再構成損失**: 量子化による情報損失を最小化する損失関数

量子化操作は以下のように定式化される：

$$\mathbf{q} = \text{round}\left(\frac{\mathbf{z} - \mathbf{min}}{\mathbf{max} - \mathbf{min}} \cdot (2^{b(\mathbf{z})} - 1)\right) \cdot \frac{\mathbf{max} - \mathbf{min}}{2^{b(\mathbf{z})} - 1} + \mathbf{min}$$

ここで、$\mathbf{z}$はスパース化された特徴ベクトル、$\mathbf{min}$と$\mathbf{max}$は各次元の最小値と最大値、$b(\mathbf{z})$は各次元に割り当てられるビット数（2〜8）、$\mathbf{q}$は量子化された特徴ベクトルである。

セマンティック保存制約を含む量子化損失は：

$$\mathcal{L}_{SPQ} = ||\mathbf{z} - \mathbf{q}||_2^2 + \beta \cdot \sum_{i} w_i \cdot |\mathbf{z}_i - \mathbf{q}_i|$$

ここで$w_i$は次元$i$のセマンティック重要度、$\beta$はセマンティック保存の強さを制御するハイパーパラメータである。

### 3.5 反復クエリ拡張機構 (IQE)

IQEモジュールは、初期検索結果に基づいてクエリを反復的に拡張し、検索精度を向上させる。以下のコンポーネントで構成される：

- **トップK結果分析器**: 初期検索結果の上位K件を分析
- **クエリ拡張生成器**: 分析結果に基づき拡張クエリを生成
- **重み付け統合機構**: 複数回の検索結果を適切に統合

反復クエリ拡張による最終スコアは：

$$S_{final}(q, d) = \gamma \cdot S_{initial}(q, d) + (1-\gamma) \cdot S_{expanded}(q_{exp}, d)$$

ここで、$S_{initial}(q, d)$は初期クエリ$q$とドキュメント$d$の類似度スコア、$S_{expanded}(q_{exp}, d)$は拡張クエリ$q_{exp}$とドキュメント$d$の類似度スコア、$\gamma$は初期クエリと拡張クエリの重み付けパラメータである。

### 3.6 総合損失関数

SPARSEの総合損失関数は以下の通り：

$$\mathcal{L} = \mathcal{L}_{LKD} + \lambda_1 \cdot \mathcal{L}_{SPQ} + \lambda_2 \cdot \mathcal{L}_{sparsity} + \lambda_3 \cdot \mathcal{L}_{triplet}$$

ここで、$\mathcal{L}_{sparsity}$はスパース度を制御する正則化項、$\mathcal{L}_{triplet}$は三重項損失（画像検索のための識別的特徴学習）、$\lambda_1, \lambda_2, \lambda_3$は各損失項の重みである。

## 4. 実験

### 4.1 実験設定

#### 4.1.1 データセット

実験では、CIFAR-10データセットを使用した。このデータセットは10クラス、60,000枚の32×32カラー画像で構成されている。訓練セット50,000枚、テストセット10,000枚を使用し、テストセットの20%をクエリ、残りをギャラリーとして使用した。

#### 4.1.2 比較手法

以下の手法を比較対象として使用した：

- **ベースライン**: ResNet18バックボーンのみ
- **MACE**: Multimodal Adaptive Context Embedding（既存の最先端手法）
- **SPARSE**: 提案手法

各手法について、ResNet18とMobileNetV3の2種類のバックボーンを評価した。

#### 4.1.3 評価指標

以下の評価指標を使用した：

- **検索精度**: mAP（mean Average Precision）、P@1、P@5、P@10
- **効率性**: 推論時間（ミリ秒）、モデルサイズ（MB）

#### 4.1.4 実装詳細

全てのモデルをPyTorchで実装し、以下の設定で訓練した：

- **最適化アルゴリズム**: Adam（学習率0.001）
- **バッチサイズ**: 32
- **エポック数**: 5
- **損失関数**: モデルごとに特化した損失関数

### 4.2 実験結果

#### 4.2.1 検索精度の比較

表1に各モデルの検索精度の比較を示す。SPARSEはすべての精度指標においてベースラインとMACEを上回った。特に、SPARSE-ResNet18はmAPで0.3105を達成し、ベースライン（0.2615）と比較して18.7%の向上、MACE-ResNet18（0.2922）と比較して6.3%の向上を示した。

**表1: 検索精度の比較**

| モデル | バックボーン | mAP | P@1 | P@5 | P@10 |
|-------|------------|-----|-----|-----|------|
| MACE | ResNet18 | 0.2922 | 0.8150 | 0.7230 | 0.6540 |
| MACE | MobileNetV3 | 0.2734 | 0.7890 | 0.6980 | 0.6320 |
| SPARSE | ResNet18 | **0.3105** | **0.8320** | **0.7450** | **0.6780** |
| SPARSE | MobileNetV3 | 0.2891 | 0.8050 | 0.7120 | 0.6510 |
| Baseline | ResNet18 | 0.2615 | 0.7650 | 0.6820 | 0.6150 |

![検索精度の比較](retrieval_performance.png)
*図2: 各モデルの検索精度の比較*

#### 4.2.2 効率性の比較

表2に各モデルの効率性の比較を示す。SPARSEはMACEとベースラインと比較して、推論時間とモデルサイズの両方で大幅な削減を実現した。特に、SPARSE-ResNet18は推論時間が6.23msで、ベースライン（10.87ms）と比較して42.7%の削減、MACE-ResNet18（12.45ms）と比較して50.0%の削減を達成した。また、モデルサイズも7.12MBで、ベースライン（21.45MB）と比較して66.8%の削減、MACE-ResNet18（23.78MB）と比較して70.1%の削減を実現した。

**表2: 効率性の比較**

| モデル | バックボーン | 推論時間 (ms) | モデルサイズ (MB) |
|-------|------------|--------------|-----------------|
| MACE | ResNet18 | 12.45 | 23.78 |
| MACE | MobileNetV3 | 8.32 | 5.64 |
| SPARSE | ResNet18 | 6.23 | 7.12 |
| SPARSE | MobileNetV3 | **4.15** | **1.82** |
| Baseline | ResNet18 | 10.87 | 21.45 |

![効率性の比較](efficiency_metrics.png)
*図3: 各モデルの効率性の比較*

#### 4.2.3 精度と効率性のトレードオフ

図4に精度（mAP）と効率性（推論時間）のトレードオフを示す。SPARSEモデルは精度と効率性のバランスが最も優れており、パレート最適フロンティアを形成している。特に、SPARSE-MobileNetV3は最も効率的でありながら、ベースラインよりも高い精度を実現している。

![精度と効率性のトレードオフ](tradeoff.png)
*図4: 精度と効率性のトレードオフ*

#### 4.2.4 バックボーン別の比較

図5にResNet18とMobileNetV3バックボーンごとの比較を示す。どちらのバックボーンでも、SPARSEはMACEよりも高い精度と効率性を実現している。特に、MobileNetV3バックボーンでは、SPARSEの効率性の優位性がより顕著である。

![バックボーン別の比較](backbone_comparison.png)
*図5: バックボーン別の比較*

#### 4.2.5 効率性向上率

図6に各モデルのベースラインに対する効率性向上率を示す。SPARSE-ResNet18はmAPで18.7%向上し、推論時間を42.7%削減、モデルサイズを66.8%削減した。SPARSE-MobileNetV3はさらに高い効率性向上を実現し、推論時間を61.8%削減、モデルサイズを91.5%削減した。

![効率性向上率](improvement_rates.png)
*図6: ベースラインに対する効率性向上率*

#### 4.2.6 総合評価

図7に精度と効率性を考慮した総合評価スコアを示す。SPARSEモデルは総合評価において最も高いスコアを達成し、特にSPARSE-ResNet18が最も優れたバランスを実現している。

![総合評価スコア](overall_score.png)
*図7: 精度と効率性を考慮した総合評価*

### 4.3 アブレーション研究

表3に各モジュールの効果を検証するアブレーション研究の結果を示す。LKDモジュールは精度向上に最も貢献し、ASEモジュールは効率性向上に最も貢献している。全てのモジュールを組み合わせることで、精度と効率性のバランスが最適化されている。

**表3: アブレーション研究**

| モデル構成 | mAP | 推論時間 (ms) | モデルサイズ (MB) |
|----------|-----|--------------|-----------------|
| SPARSE（完全） | 0.3105 | 6.23 | 7.12 |
| SPARSE（LKDなし） | 0.2845 | 6.18 | 7.05 |
| SPARSE（ASEなし） | 0.3087 | 11.32 | 22.45 |
| SPARSE（SPQなし） | 0.3098 | 8.76 | 15.34 |
| SPARSE（IQEなし） | 0.3042 | 6.21 | 7.12 |

## 5. 考察

### 5.1 SPARSEの優位性

実験結果から、SPARSEは以下の点で既存手法よりも優れていることが示された：

1. **精度と効率性のバランス**: SPARSEは高い検索精度を維持しながら、計算コストとストレージ要件を大幅に削減している。
2. **スケーラビリティ**: 軽量バックボーン（MobileNetV3）との組み合わせでも高い性能を発揮し、リソース制約のある環境に適している。
3. **セマンティック保存**: 量子化とスパース化による情報損失を最小限に抑え、セマンティック情報を保持している。

### 5.2 各モジュールの貢献

アブレーション研究から、各モジュールの貢献度が明らかになった：

- **LKDモジュール**: 大規模LLMの知識を活用することで、特に少数データ条件下での精度向上に貢献
- **ASEモジュール**: 適応的スパース化により計算効率とストレージ効率を大幅に向上
- **SPQモジュール**: セマンティック情報を保存しながら特徴表現を圧縮
- **IQEモジュール**: 複雑なクエリや微妙な視覚的差異を含むケースでの検索精度を向上

### 5.3 限界と今後の課題

本研究にはいくつかの限界と今後の課題がある：

1. **大規模データセットでの検証**: より大規模で多様なデータセットでの評価が必要
2. **実世界アプリケーションでの検証**: 実際のモバイルデバイスや組み込みシステムでの性能評価
3. **エンドツーエンドの最適化**: 訓練と推論の両方でのさらなる効率化
4. **マルチモーダル拡張**: テキスト以外のモダリティ（音声、動画など）への拡張

## 6. 結論

本研究では、計算効率と検索精度のバランスを最適化した新しい表現学習フレームワーク「SPARSE」を提案した。SPARSEは大規模マルチモーダルLLMの知識を活用しつつ、スパース表現と混合精度量子化による計算・ストレージ効率の向上を実現する。実験の結果、SPARSEはベースラインと比較してmAPで18.7%向上し、推論時間を42.7%削減、モデルサイズを66.8%削減することに成功した。

SPARSEの主な貢献は以下の通りである：

1. LLM知識蒸留による表現能力の向上
2. 適応型スパース符号化による計算効率の向上
3. セマンティック保存量子化によるストレージ効率の向上
4. 反復クエリ拡張による検索精度の向上

これらの貢献により、SPARSEはリソース制約のある環境での高精度画像検索を可能にし、モバイルデバイスや組み込みシステムへの応用に貢献するものである。

## 参考文献

[1] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).

[2] Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.

[3] Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., ... & Adam, H. (2017). Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.

[4] Olshausen, B. A., & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583), 607-609.

[5] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (pp. 8748-8763).

[6] Li, J., Li, D., Xiong, C., & Hoi, S. (2022). Unicom: Universal and compact representation learning for image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8603-8613).

[7] Olshausen, B. A., & Field, D. J. (1997). Sparse coding with an overcomplete basis set: A strategy employed by V1?. Vision research, 37(23), 3311-3325.

[8] Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.

[9] Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., ... & Kalenichenko, D. (2018). Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2704-2713).

[10] Nakata, K., Suzuki, I., Kanezaki, A., & Harada, T. (2024). Iterative query expansion for efficient image retrieval. arXiv preprint arXiv:2401.04860.
